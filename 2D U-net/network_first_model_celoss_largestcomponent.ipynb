{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3bc758-73bf-4010-b6d8-2079d6756aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 20:56:29.422756: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-24 20:56:29.461367: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-24 20:56:29.461382: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-24 20:56:29.461404: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-24 20:56:29.469201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-24 20:56:30.293932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1526 2D training samples.\n",
      "Found 376 2D validation samples.\n",
      "Epoch 1/10, Average Training Loss: 1.0136\n",
      "Epoch 1/10, Validation Loss: 0.8631\n",
      "Epoch 2/10, Average Training Loss: 0.7721\n",
      "Epoch 2/10, Validation Loss: 0.6838\n",
      "Epoch 3/10, Average Training Loss: 0.5660\n",
      "Epoch 3/10, Validation Loss: 0.4691\n",
      "Epoch 4/10, Average Training Loss: 0.3741\n",
      "Epoch 4/10, Validation Loss: 0.3181\n",
      "Epoch 5/10, Average Training Loss: 0.2565\n",
      "Epoch 5/10, Validation Loss: 0.2333\n",
      "Epoch 6/10, Average Training Loss: 0.1887\n",
      "Epoch 6/10, Validation Loss: 0.1742\n",
      "Epoch 7/10, Average Training Loss: 0.1463\n",
      "Epoch 7/10, Validation Loss: 0.1461\n",
      "Epoch 8/10, Average Training Loss: 0.1170\n",
      "Epoch 8/10, Validation Loss: 0.1254\n",
      "Epoch 9/10, Average Training Loss: 0.0966\n",
      "Epoch 9/10, Validation Loss: 0.1068\n",
      "Epoch 10/10, Average Training Loss: 0.0827\n",
      "Epoch 10/10, Validation Loss: 0.0869\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from monai.networks.nets import UNet\n",
    "from monai.data import Dataset, DataLoader, pad_list_data_collate\n",
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, ToTensord, ResizeD\n",
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. Prepare the 2D Datasets and DataLoaders\n",
    "# ------------------------------------------------\n",
    "\n",
    "train_2d_dir = './dataset/train_2d'\n",
    "val_2d_dir = './dataset/val_2d'\n",
    "\n",
    "train_data_2d = []\n",
    "for patient in os.listdir(train_2d_dir):\n",
    "    patient_path = os.path.join(train_2d_dir, patient)\n",
    "    if os.path.isdir(patient_path):\n",
    "        for file in os.listdir(patient_path):\n",
    "            if file.endswith('.nii') and '_gt' not in file:\n",
    "                image_path = os.path.join(patient_path, file)\n",
    "                if '_slice' in file:\n",
    "                    parts = file.split('_slice')\n",
    "                    gt_file = parts[0] + '_gt_slice' + parts[1]\n",
    "                else:\n",
    "                    gt_file = file.replace('.nii', '_gt.nii')\n",
    "                gt_path = os.path.join(patient_path, gt_file)\n",
    "                if os.path.exists(gt_path):\n",
    "                    train_data_2d.append({\"image\": image_path, \"label\": gt_path})\n",
    "                else:\n",
    "                    print(\"Warning: No corresponding ground truth for\", image_path)\n",
    "print(f\"Found {len(train_data_2d)} 2D training samples.\")\n",
    "\n",
    "val_data_2d = []\n",
    "for patient in os.listdir(val_2d_dir):\n",
    "    patient_path = os.path.join(val_2d_dir, patient)\n",
    "    if os.path.isdir(patient_path):\n",
    "        for file in os.listdir(patient_path):\n",
    "            if file.endswith('.nii') and '_gt' not in file:\n",
    "                image_path = os.path.join(patient_path, file)\n",
    "                if '_slice' in file:\n",
    "                    parts = file.split('_slice')\n",
    "                    gt_file = parts[0] + '_gt_slice' + parts[1]\n",
    "                else:\n",
    "                    gt_file = file.replace('.nii', '_gt.nii')\n",
    "                gt_path = os.path.join(patient_path, gt_file)\n",
    "                if os.path.exists(gt_path):\n",
    "                    val_data_2d.append({\"image\": image_path, \"label\": gt_path})\n",
    "                else:\n",
    "                    print(\"Warning: No corresponding ground truth for\", image_path)\n",
    "print(f\"Found {len(val_data_2d)} 2D validation samples.\")\n",
    "\n",
    "transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    ResizeD(keys=[\"image\", \"label\"], spatial_size=(352, 352)),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "train_ds_2d = Dataset(data=train_data_2d, transform=transforms)\n",
    "val_ds_2d = Dataset(data=val_data_2d, transform=transforms)\n",
    "\n",
    "train_loader_2d = DataLoader(train_ds_2d, batch_size=10, shuffle=True, collate_fn=pad_list_data_collate)\n",
    "val_loader_2d = DataLoader(val_ds_2d, batch_size=10, shuffle=False, collate_fn=pad_list_data_collate)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. Build the 2D U-Net Model and Training Setup\n",
    "# ------------------------------------------------\n",
    "\n",
    "in_channels = 1\n",
    "out_channels = 4\n",
    "initial_features = 48\n",
    "\n",
    "channels = (initial_features, initial_features*2, initial_features*4, initial_features*8)\n",
    "strides = (2, 2, 2)\n",
    "num_res_units = 2\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    channels=channels,\n",
    "    strides=strides,\n",
    "    num_res_units=num_res_units,\n",
    "    norm=\"batch\",\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Weighted Cross-Entropy Loss Setup\n",
    "# ------------------------------------------------\n",
    "class_weights = torch.tensor([0.2, 0.3, 0.3, 0.2], device=device, dtype=torch.float)\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. Utility: Keep Largest Connected Component\n",
    "# ------------------------------------------------\n",
    "def keep_largest_component(pred_mask):\n",
    "    \"\"\"\n",
    "    Given a 2D predicted mask (H, W) with integer labels,\n",
    "    return a new mask where for each non-background class (label != 0),\n",
    "    only the largest connected component is kept.\n",
    "    \"\"\"\n",
    "    output = np.zeros_like(pred_mask)\n",
    "    for cls in np.unique(pred_mask):\n",
    "        if cls == 0:\n",
    "            continue  # skip background\n",
    "        # create binary mask for this class\n",
    "        binary_mask = (pred_mask == cls).astype(np.int32)\n",
    "        labeled_array, num_features = label(binary_mask)\n",
    "        if num_features == 0:\n",
    "            continue\n",
    "        sizes = np.bincount(labeled_array.ravel())\n",
    "        sizes[0] = 0  # ignore background\n",
    "        largest_label = sizes.argmax()\n",
    "        largest_component = (labeled_array == largest_label)\n",
    "        output[largest_component] = cls\n",
    "    return output\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. Training Loop with Validation Postprocessing\n",
    "# ------------------------------------------------\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_data in train_loader_2d:\n",
    "        inputs = batch_data[\"image\"].to(device)   # (B, 1, 352, 352)\n",
    "        labels = batch_data[\"label\"].to(device)     # (B, 1, 352, 352)\n",
    "        labels = labels.squeeze(1).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)                     # (B, 4, 352, 352)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader_2d)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Validation loop with postprocessing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for val_data in val_loader_2d:\n",
    "            val_inputs = val_data[\"image\"].to(device)\n",
    "            val_labels = val_data[\"label\"].to(device).squeeze(1).long()\n",
    "            val_outputs = model(val_inputs)\n",
    "            loss_val = loss_function(val_outputs, val_labels)\n",
    "            val_loss += loss_val.item()\n",
    "            \n",
    "            # Obtain predictions (before postprocessing).\n",
    "            preds_batch = torch.argmax(val_outputs, dim=1)  # shape: (B, 352, 352)\n",
    "            \n",
    "            # Now apply postprocessing per sample: keep largest connected component for each class.\n",
    "            preds_np = preds_batch.cpu().numpy()\n",
    "            processed_preds = []\n",
    "            for pred in preds_np:\n",
    "                # Process each 2D slice individually.\n",
    "                processed_pred = keep_largest_component(pred)\n",
    "                processed_preds.append(processed_pred)\n",
    "            # You can further evaluate the processed_preds if desired.\n",
    "            \n",
    "        avg_val_loss = val_loss / len(val_loader_2d)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908d3a03-8bc7-400f-a28a-e515263b0213",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2025-02-24 21:56:25.665484: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-24 21:56:25.703519: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-24 21:56:25.703539: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-24 21:56:25.703556: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-24 21:56:25.710885: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-24 21:56:26.530837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiceMetric, HausdorffDistanceMetric\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1. Save the model checkpoint.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2d_unet_checkpoint_first_model_celoss_largestcomponent.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel checkpoint saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 2. Visualize predictions on a validation batch.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "\n",
    "# 1. Save the model checkpoint.\n",
    "torch.save(model.state_dict(), \"2d_unet_checkpoint_first_model_celoss_largestcomponent.pth\")\n",
    "print(\"Model checkpoint saved.\")\n",
    "\n",
    "# 2. Visualize predictions on a validation batch.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(val_loader_2d))\n",
    "    inputs = batch[\"image\"].to(device)\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    outputs = model(inputs)\n",
    "    # Get the predicted segmentation as the argmax over the channel dimension.\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Move inputs and labels to CPU for visualization.\n",
    "inputs = inputs.cpu().numpy()   # shape: (B, 1, H, W)\n",
    "labels = labels.cpu().numpy()   # shape: (B, 1, H, W)\n",
    "\n",
    "# Number of samples to display.\n",
    "num_to_show = min(6, inputs.shape[0])\n",
    "fig, axs = plt.subplots(3, num_to_show, figsize=(4*num_to_show, 12))\n",
    "for i in range(num_to_show):\n",
    "    # Input image.\n",
    "    axs[0, i].imshow(inputs[i, 0, :, :], cmap=\"gray\")\n",
    "    axs[0, i].set_title(f\"Input {i}\")\n",
    "    axs[0, i].axis(\"off\")\n",
    "    # Ground truth.\n",
    "    axs[1, i].imshow(labels[i, 0, :, :], cmap=\"jet\")\n",
    "    axs[1, i].set_title(f\"Ground Truth {i}\")\n",
    "    axs[1, i].axis(\"off\")\n",
    "    # Prediction.\n",
    "    axs[2, i].imshow(preds[i, :, :], cmap=\"jet\")\n",
    "    axs[2, i].set_title(f\"Prediction {i}\")\n",
    "    axs[2, i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Evaluate the model on the full validation set.\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "hd_metric = HausdorffDistanceMetric(include_background=True, percentile=95)\n",
    "\n",
    "total_correct = 0\n",
    "total_pixels = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for val_batch in val_loader_2d:\n",
    "        val_inputs = val_batch[\"image\"].to(device)\n",
    "        val_labels = val_batch[\"label\"].to(device)\n",
    "        val_outputs = model(val_inputs)\n",
    "        # Obtain predictions.\n",
    "        preds_batch = torch.argmax(val_outputs, dim=1, keepdim=True)  # shape: (B, 1, H, W)\n",
    "        # Update metrics.\n",
    "        dice_metric(y_pred=preds_batch, y=val_labels)\n",
    "        hd_metric(y_pred=preds_batch, y=val_labels)\n",
    "        # Pixel-wise accuracy.\n",
    "        total_correct += (preds_batch == val_labels).sum().item()\n",
    "        total_pixels += torch.numel(val_labels)\n",
    "\n",
    "dice_score = dice_metric.aggregate().item()\n",
    "hd_score = hd_metric.aggregate().item()\n",
    "accuracy = total_correct / total_pixels\n",
    "\n",
    "print(f\"Dice Score: {dice_score:.4f}\")\n",
    "print(f\"Hausdorff Distance (95th percentile): {hd_score:.4f}\")\n",
    "print(f\"Pixel-wise Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
